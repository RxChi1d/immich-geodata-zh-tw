"""å—éŸ“åœ°ç†è³‡æ–™è™•ç†å™¨ã€‚"""

import re

import polars as pl
import geopandas as gpd
import pyproj
import numpy as np
from collections.abc import Callable
from pathlib import Path

from core.utils import logger
from core.utils.wikidata_translator import WikidataTranslator
from core.geodata.base import GeoDataHandler, register_handler


@register_handler("KR")
class SouthKoreaGeoDataHandler(GeoDataHandler):
    """å—éŸ“åœ°ç†è³‡æ–™è™•ç†å™¨ã€‚

    è³‡æ–™ä¾†æºï¼šhttps://github.com/vuski/admdongkor
    ä½¿ç”¨å‹•æ…‹ UTM å€é¸æ“‡æ–¹æ³•ï¼ˆçµåˆ Albers æŠ•å½±ï¼‰è¨ˆç®—ä¸­å¿ƒé»ã€‚
    """

    COUNTRY_NAME = "å—éŸ“"
    COUNTRY_CODE = "KR"
    TIMEZONE = "Asia/Seoul"

    CITY_DISTRICT_PATTERN = re.compile(r"^(?P<city>.+?ì‹œ)(?P<district>.+?(?:êµ¬|êµ°))$")

    # å»£åŸŸå¸‚/é“åç¨±å°ç…§è¡¨ï¼ˆéŸ“æ–‡ â†’ å°ç£å¸¸ç”¨ç¹é«”ä¸­æ–‡åç¨±ï¼‰
    # Reason: ä½¿ç”¨å°ç£åœ°åœ–å¸¸è¦‹çš„ç°¡æ½”åç¨±ï¼Œè€Œé Google Maps çš„æ­£å¼å®˜æ–¹åç¨±
    ADMIN1_NAME_MAP = {
        # ç‰¹åˆ¥å¸‚
        "ì„œìš¸íŠ¹ë³„ì‹œ": "é¦–çˆ¾",
        # å»£åŸŸå¸‚ï¼ˆ6å€‹ï¼‰
        "ë¶€ì‚°ê´‘ì—­ì‹œ": "é‡œå±±",
        "ëŒ€êµ¬ê´‘ì—­ì‹œ": "å¤§é‚±",
        "ì¸ì²œê´‘ì—­ì‹œ": "ä»å·",
        "ê´‘ì£¼ê´‘ì—­ì‹œ": "å…‰å·",
        "ëŒ€ì „ê´‘ì—­ì‹œ": "å¤§ç”°",
        "ìš¸ì‚°ê´‘ì—­ì‹œ": "è”šå±±",
        # ç‰¹åˆ¥è‡ªæ²»å¸‚
        "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ": "ä¸–å®—",
        # é“ï¼ˆ8å€‹ï¼‰
        "ê²½ê¸°ë„": "äº¬ç•¿é“",
        "ê°•ì›íŠ¹ë³„ìì¹˜ë„": "æ±ŸåŸé“",
        "ì¶©ì²­ë¶ë„": "å¿ æ¸…åŒ—é“",
        "ì¶©ì²­ë‚¨ë„": "å¿ æ¸…å—é“",
        "ì „ë¶íŠ¹ë³„ìì¹˜ë„": "å…¨ç¾…åŒ—é“",
        "ì „ë¼ë‚¨ë„": "å…¨ç¾…å—é“",
        "ê²½ìƒë¶ë„": "æ…¶å°šåŒ—é“",
        "ê²½ìƒë‚¨ë„": "æ…¶å°šå—é“",
        "ì œì£¼íŠ¹ë³„ìì¹˜ë„": "æ¿Ÿå·",
    }

    # ä¸–å®—ç‰¹åˆ¥è‡ªæ²»å¸‚ Admin_2 æ‰‹å‹•ç¿»è­¯å°ç…§è¡¨ï¼ˆéŸ“æ–‡ â†’ ç¹é«”ä¸­æ–‡ï¼‰
    # Reason: Wikidata ç¼ºå°‘é€™äº›æ–°è¨­ç«‹è¡Œæ”¿å€ï¼ˆ2012 å¹´å¾Œï¼‰çš„ç¹é«”ä¸­æ–‡æ¨™ç±¤
    #         ç›´æ¥ä½¿ç”¨å°ç…§è¡¨é¿å…ç„¡æ•ˆæŸ¥è©¢ï¼Œæå‡æ•ˆèƒ½ä¸¦ç¢ºä¿ 100% ç¹é«”ä¸­æ–‡è¦†è“‹ç‡
    SEJONG_ADMIN2_MAP = {
        # 8 å€‹æ´ (Dong) - Wikidata ç„¡ zh-tw æ¨™ç±¤
        "ë³´ëŒë™": "å¯¶è—æ´",
        "ëŒ€í‰ë™": "å¤§åªæ´",
        "ë‹¤ì •ë™": "å¤šæƒ…æ´",
        "ë„ë‹´ë™": "å¶‹æ½­æ´",
        "ê³ ìš´ë™": "é«˜é‹æ´",
        "ì¢…ì´Œë™": "é¾æ‘æ´",
        "ìƒˆë¡¬ë™": "æ–°ç¾…æ´",
        "ì†Œë‹´ë™": "ç´ æ½­æ´",
        # 3 å€‹å…¶ä»–éŸ“æ–‡åœ°å - CSV ä¸­ä»ç‚ºéŸ“æ–‡
        "ì–´ì§„ë™": "å¾¡çæ´",
        "ë°˜ê³¡ë™": "ç›¤è°·æ´",
        "í•´ë°€ë™": "æµ·å¯†æ´",
        # ä»¥ä¸‹ç‚ºå·²å¯é€šé Wikidata ç¿»è­¯çš„åœ°åï¼ˆå¯é¸ï¼Œé¿å…é‡è¤‡æŸ¥è©¢æå‡æ•ˆèƒ½ï¼‰
        "ì¡°ì¹˜ì›ì": "é³¥è‡´é™¢é‚‘",
        "ë¶€ê°•ë©´": "èŠ™æ±Ÿé¢",
        "ì¥êµ°ë©´": "å°‡è»é¢",
        "ì—°ì„œë©´": "ç‡•è¥¿é¢",
        "ì „ì˜ë©´": "å…¨ç¾©é¢",
        "ì „ë™ë©´": "å…¨æ±é¢",
        "ì†Œì •ë©´": "å°äº•é¢",
        "í•œì†”ë™": "æ‰ç‡æ´",
        "í•©ê°•ë™": "åˆæŠ±æ´",
        "ì—°ê¸°ë©´": "ç‡•å²é¢",
        "ì—°ë™ë©´": "ç‡•æ±é¢",
        "ê¸ˆë‚¨ë©´": "éŒ¦å—é¢",
        "ë‚˜ì„±ë™": "ç¾…åŸæ´",
        "ì•„ë¦„ë™": "é˜¿å‡œæ´",
    }

    def _get_utm_epsg_from_lon(self, longitude: float) -> int:
        """æ ¹æ“šç¶“åº¦è¨ˆç®— UTM å€çš„ EPSG ä»£ç¢¼ã€‚"""
        zone = int((longitude + 180) / 6) + 1
        return 32600 + zone

    def _calculate_centroids_utm(self, gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:
        """ä½¿ç”¨å‹•æ…‹ UTM å€é¸æ“‡è¨ˆç®—ä¸­å¿ƒé»ï¼ˆå‘é‡åŒ–ï¼‰ã€‚

        çµåˆ Albers æŠ•å½±å’Œå‹•æ…‹ UTM å€é¸æ“‡ï¼Œæä¾›é«˜ç²¾ç¢ºåº¦çš„ä¸­å¿ƒé»è¨ˆç®—ã€‚
        """
        # ç¢ºä¿ä½¿ç”¨ WGS84 åº§æ¨™ç³»çµ±
        if gdf.crs.to_epsg() != 4326:
            logger.info("æ­£åœ¨è½‰æ›åˆ° WGS84...")
            gdf = gdf.to_crs(epsg=4326)

        # ä½¿ç”¨ Albers æŠ•å½±è¨ˆç®—æº–ç¢ºçš„ä¸­å¿ƒé»ç¶“åº¦
        # Reason: é‚Šç•Œæ¡†å¹³å‡å€¼å°æ–¼ä¸è¦å‰‡å½¢ç‹€å¯èƒ½ä¸æº–ç¢ºï¼Œ
        #         ç‰¹åˆ¥æ˜¯åœ¨ UTM å€é‚Šç•Œé™„è¿‘ï¼ˆå—éŸ“æ©«è·¨ 126Â°Eï¼‰
        logger.info("æ­£åœ¨è¨ˆç®—æº–ç¢ºçš„å¹¾ä½•ä¸­å¿ƒé»ï¼ˆä½¿ç”¨ Albers æŠ•å½±ï¼‰...")

        # å®šç¾©ä»¥å—éŸ“ç‚ºä¸­å¿ƒçš„ Albers ç­‰é¢ç©åœ“éŒæŠ•å½±
        korea_albers = pyproj.CRS.from_proj4(
            "+proj=aea +lat_1=33 +lat_2=43 +lat_0=37 +lon_0=127.5 "
            "+x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs"
        )

        # æŠ•å½±åˆ° Albers ä¸¦è¨ˆç®—ä¸­å¿ƒé»
        gdf_albers = gdf.to_crs(korea_albers)
        centroids_albers = gdf_albers.geometry.centroid

        # å°‡ä¸­å¿ƒé»è½‰å› WGS84 ä»¥å–å¾—æº–ç¢ºçš„ç¶“åº¦
        centroids_wgs84_temp = centroids_albers.to_crs(epsg=4326)
        center_lons = centroids_wgs84_temp.x

        # æ ¹æ“šæº–ç¢ºçš„ä¸­å¿ƒé»ç¶“åº¦è¨ˆç®— UTM å€ï¼ˆå‘é‡åŒ–ï¼‰
        logger.info("æ­£åœ¨æ ¹æ“šä¸­å¿ƒé»ç¶“åº¦æ±ºå®š UTM å€...")
        utm_zones = ((center_lons + 180) / 6).astype(int) + 1
        utm_epsgs = 32600 + utm_zones

        # å°‡ UTM å€è³‡è¨ŠåŠ å…¥ GeoDataFrame
        gdf["_utm_zone"] = utm_zones
        gdf["_utm_epsg"] = utm_epsgs

        logger.info(f"è­˜åˆ¥åˆ° {utm_epsgs.nunique()} å€‹ä¸åŒçš„ UTM å€")

        # å»ºç«‹é™£åˆ—å„²å­˜çµæœï¼ˆåˆå§‹åŒ–ç‚º NaNï¼‰
        longitudes = np.full(len(gdf), np.nan)
        latitudes = np.full(len(gdf), np.nan)

        # æŒ‰ UTM å€æ‰¹æ¬¡è™•ç†ï¼ˆä¾ UTM EPSG åˆ†çµ„ï¼‰
        # Reason: æ¯å€‹ UTM å€éœ€è¦ä¸åŒçš„æŠ•å½±ï¼Œ
        #         ä½†åœ¨æ¯å€‹å€å…§æˆ‘å€‘ä¸€æ¬¡è™•ç†æ‰€æœ‰å¹¾ä½•é«”ï¼ˆå‘é‡åŒ–ï¼‰
        logger.info("æ­£åœ¨æŒ‰ UTM å€æ‰¹æ¬¡è¨ˆç®—ä¸­å¿ƒé»...")
        for utm_epsg, group_idx in gdf.groupby("_utm_epsg").groups.items():
            # å–å¾—æ­¤ UTM å€çš„å¹¾ä½•é«”
            group_gdf = gdf.iloc[group_idx]

            # è½‰æ›åˆ° UTM æŠ•å½±ï¼ˆæ‰¹æ¬¡æ“ä½œï¼Œéè¿´åœˆï¼‰
            group_utm = group_gdf.to_crs(epsg=utm_epsg)

            # åœ¨ UTM ä¸­è¨ˆç®—ä¸­å¿ƒé»ï¼ˆå‘é‡åŒ–ï¼‰
            centroids_utm = group_utm.geometry.centroid

            # è½‰å› WGS84ï¼ˆæ‰¹æ¬¡æ“ä½œï¼‰
            centroids_wgs84 = centroids_utm.to_crs(epsg=4326)

            # ä½¿ç”¨å‘é‡åŒ–çš„ .x å’Œ .y å±¬æ€§æå–åº§æ¨™
            # Reason: ç›´æ¥ä½¿ç”¨ NumPy é™£åˆ—é‹ç®—ï¼Œç„¡ Python è¿´åœˆ
            longitudes[group_idx] = centroids_wgs84.x.values
            latitudes[group_idx] = centroids_wgs84.y.values

        # å°‡åº§æ¨™åŠ å…¥ GeoDataFrameï¼ˆå‘é‡åŒ–è³¦å€¼ï¼‰
        gdf["longitude"] = longitudes
        gdf["latitude"] = latitudes

        # æ¸…ç†æš«å­˜æ¬„ä½
        gdf = gdf.drop(columns=["_utm_zone", "_utm_epsg"])

        return gdf

    def _normalize_special_admin_structures(self, df: pl.DataFrame) -> pl.DataFrame:
        """æ­£è¦åŒ–ç‰¹æ®Šè¡Œæ”¿å€çµæ§‹ï¼ˆå¦‚ä¸–å®—ç‰¹åˆ¥è‡ªæ²»å¸‚ï¼‰ã€‚

        ä¸–å®—ç‰¹åˆ¥è‡ªæ²»å¸‚æ˜¯å—éŸ“å”¯ä¸€çš„å–®å±¤åˆ¶ç‰¹åˆ¥è‡ªæ²»å¸‚ï¼Œæ²’æœ‰å‚³çµ±çš„å¸‚/éƒ¡/å€å±¤ç´šã€‚
        è¡Œæ”¿å±¤ç´šç›´æ¥å¾å»£åŸŸå¸‚åˆ°è®€/é¢/æ´ã€‚

        ç‚ºäº†ç¢ºä¿ cities500 è³‡æ–™çš„ name æ¬„ä½æœ‰å€¼ï¼ˆé è¨­ä½¿ç”¨ admin_2ï¼‰ï¼Œ
        éœ€è¦å°‡ admin_3ï¼ˆì/ë©´/ë™ï¼‰ä¸Šç§»åˆ° admin_2ï¼Œä»¥ä¾¿ç¿»è­¯å’Œé¡¯ç¤ºã€‚

        Args:
            df: åŒ…å« sidonm, sggnm, admin_3 æ¬„ä½çš„ DataFrame

        Returns:
            æ­£è¦åŒ–å¾Œçš„ DataFrame
        """
        # æª¢æ¸¬æ¢ä»¶ï¼šsidonm == "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ" ä¸” sggnm ä¸æ˜¯çœŸå¯¦çš„è¡Œæ”¿å€åç¨±
        # Reason: ä¸–å®—çš„çœŸå¯¦è¡Œæ”¿å€æ‡‰è©²æ˜¯ì/ë©´/ë™ï¼Œå¦‚æœä¸æ˜¯å°±è¡¨ç¤ºæ˜¯æ©Ÿæ§‹åç¨±ï¼ˆè­°æœƒã€å¸‚å»³ç­‰ï¼‰
        sejong_mask = (pl.col("sidonm") == "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ") & (
            ~pl.col("sggnm").str.ends_with("ì")
            & ~pl.col("sggnm").str.ends_with("ë©´")
            & ~pl.col("sggnm").str.ends_with("ë™")
        )

        sejong_count = df.filter(sejong_mask).height
        if sejong_count > 0:
            logger.info(
                f"åµæ¸¬åˆ° {sejong_count} ç­†ä¸–å®—ç‰¹åˆ¥è‡ªæ²»å¸‚è¨˜éŒ„ï¼Œæ­£åœ¨æ­£è¦åŒ–è¡Œæ”¿å±¤ç´š..."
            )

            df = df.with_columns(
                [
                    # å°‡ admin_3 ä¸Šç§»åˆ° sggnm
                    pl.when(sejong_mask)
                    .then(pl.col("admin_3"))
                    .otherwise(pl.col("sggnm"))
                    .alias("sggnm"),
                    # å°‡åŸ admin_3 æ¸…ç©ºï¼ˆä¸–å®—æ²’æœ‰æ›´ä¸‹å±¤ç´šï¼‰
                    pl.when(sejong_mask)
                    .then(pl.lit(None, dtype=pl.String))
                    .otherwise(pl.col("admin_3"))
                    .alias("admin_3"),
                ]
            )

            logger.info(
                "ä¸–å®—ç‰¹åˆ¥è‡ªæ²»å¸‚è¡Œæ”¿å±¤ç´šæ­£è¦åŒ–å®Œæˆï¼šì/ë©´/ë™ å·²ä¸Šç§»åˆ° admin_2 å±¤ç´š"
            )

        return df

    def _split_city_district_name(self, name: str) -> tuple[str, str]:
        """æ‹†åˆ†éŸ“æ–‡çš„ã€Œå¸‚ï¼‹å€/éƒ¡ã€åˆä½µåç¨±ã€‚

        Args:
            name: sggnm æ¬„ä½ä¸­çš„éŸ“æ–‡åç¨±

        Returns:
            ä»¥ (city, district) å½¢å¼å›å‚³ï¼Œè‹¥ç„¡æ³•æ‹†åˆ†å‰‡ district ç‚ºç©ºå­—ä¸²
        """

        if not name or "ì‹œ" not in name:
            return name, ""

        match = self.CITY_DISTRICT_PATTERN.match(name)
        if not match:
            return name, ""

        city = match.group("city")
        district = match.group("district")
        if not district.endswith(("êµ¬", "êµ°")):
            return name, ""

        return city, district

    def _normalize_city_district_hierarchy(self, df: pl.DataFrame) -> pl.DataFrame:
        """å°‡å¸‚ï¼‹å€åˆä½µåç¨±æ‹†åˆ†ä¸¦èª¿æ•´ admin å±¤ç´šã€‚"""

        if "sggnm" not in df.columns:
            return df

        if "admin_4" not in df.columns:
            df = df.with_columns(pl.lit(None, dtype=pl.String).alias("admin_4"))

        df = df.with_columns(
            [
                pl.col("sggnm")
                .map_elements(
                    lambda name: self._split_city_district_name(name)[0],
                    return_dtype=pl.String,
                )
                .alias("_city_part"),
                pl.col("sggnm")
                .map_elements(
                    lambda name: self._split_city_district_name(name)[1],
                    return_dtype=pl.String,
                )
                .alias("_district_part"),
            ]
        )

        split_mask = pl.col("_district_part") != ""
        split_count = df.filter(split_mask).height
        if split_count > 0:
            logger.info(f"åµæ¸¬åˆ° {split_count} ç­†å¸‚ï¼‹å€åˆä½µåç¨±ï¼Œæ­£åœ¨æ‹†åˆ†éšå±¤...")

        df = df.with_columns(
            [
                pl.when(split_mask)
                .then(pl.col("_city_part"))
                .otherwise(pl.col("sggnm"))
                .alias("sggnm"),
                pl.when(split_mask)
                .then(pl.col("_district_part"))
                .otherwise(pl.col("admin_3"))
                .alias("admin_3"),
                pl.when(split_mask)
                .then(pl.col("admin_3"))
                .otherwise(pl.col("admin_4"))
                .alias("admin_4"),
            ]
        )

        return df.drop(["_city_part", "_district_part"])

    @staticmethod
    def _build_candidate_filter() -> Callable[[str, dict], bool]:
        """å»ºç«‹å€™é¸éæ¿¾å™¨ï¼Œæ’é™¤è­°æœƒæ©Ÿæ§‹ç­‰éè¡Œæ”¿å€å¯¦é«”ã€‚

        Returns:
            éæ¿¾å™¨å‡½å¼ï¼Œæ¥æ”¶ (name, metadata) ä¸¦å›å‚³ bool
        """
        # éœ€è¦æ’é™¤çš„é—œéµå­—ï¼ˆå¤šèªè¨€ï¼‰
        # Reason: é˜²æ­¢å°‡è­°æœƒæ©Ÿæ§‹ã€æ”¿åºœæ©Ÿé—œèª¤åˆ¤ç‚ºè¡Œæ”¿å€ï¼›åƒ…ä¿ç•™å®Œæ•´è©å½™ï¼Œ
        #         é¿å…åƒã€Œì²­ã€é€™é¡å–®å­—èª¤å‚·åˆæ³•åœ°åï¼ˆä¾‹ï¼šæ¸…é“éƒ¡ï¼‰
        EXCLUDED_KEYWORDS = [
            "ì˜íšŒ",  # éŸ“æ–‡ï¼šè­°æœƒ
            "è­°æœƒ",  # ä¸­æ–‡ï¼šè­°æœƒ
            "council",  # è‹±æ–‡ï¼šè­°æœƒ
            "assembly",  # è‹±æ–‡ï¼šè­°æœƒ
            "å§”å“¡æœƒ",  # ä¸­æ–‡ï¼šå§”å“¡æœƒ
            "legislature",  # è‹±æ–‡ï¼šç«‹æ³•æ©Ÿæ§‹
            "å»³",  # ä¸­æ–‡ï¼šå»³
            "government",  # è‹±æ–‡ï¼šæ”¿åºœ
            "êµìœ¡ì²­",  # éŸ“æ–‡ï¼šæ•™è‚²å»³
            "ë„ì²­",  # éŸ“æ–‡ï¼šé“å»³
            "êµ°ì²­",  # éŸ“æ–‡ï¼šéƒ¡å»³
            "êµ¬ì²­",  # éŸ“æ–‡ï¼šå€å…¬æ‰€
            "ì‹œì²­",  # éŸ“æ–‡ï¼šå¸‚å»³
        ]

        def filter_func(name: str, metadata: dict) -> bool:
            """éæ¿¾å€™é¸é …ï¼šæ’é™¤åŒ…å«è­°æœƒç›¸é—œé—œéµå­—çš„å€™é¸ã€‚

            Args:
                name: åœ°åï¼ˆæœªä½¿ç”¨ï¼Œä¿ç•™ä»¥ç¬¦åˆä»‹é¢ï¼‰
                metadata: åŒ…å« qid å’Œ labels çš„å­—å…¸

            Returns:
                True ä¿ç•™æ­¤å€™é¸ï¼ŒFalse æ’é™¤æ­¤å€™é¸
            """
            labels = metadata.get("labels", {})

            # æª¢æŸ¥æ‰€æœ‰èªè¨€çš„æ¨™ç±¤
            for lang_code, label in labels.items():
                label_lower = label.lower()
                for keyword in EXCLUDED_KEYWORDS:
                    if keyword.lower() in label_lower:
                        logger.debug(
                            f"éæ¿¾æ‰å€™é¸ {metadata.get('qid')}: "
                            f"æ¨™ç±¤ [{lang_code}] '{label}' åŒ…å«é—œéµå­— '{keyword}'"
                        )
                        return False  # æ’é™¤æ­¤å€™é¸

            return True  # ä¿ç•™æ­¤å€™é¸

        return filter_func

    def extract_from_shapefile(
        self,
        shapefile_path: str,
        output_csv: str,
    ) -> None:
        """å¾å—éŸ“è¡Œæ”¿å€ GeoJSON æå–åœ°ç†è³‡æ–™ä¸¦è½‰æ›ç‚ºæ¨™æº–åŒ– CSVã€‚

        è™•ç†å—éŸ“è¡Œæ”¿å€åŸŸè³‡æ–™ï¼Œè¨ˆç®—ä¸­å¿ƒé»åº§æ¨™ä¸¦æŒ‰ç…§è¡Œæ”¿å€å±¤ç´šæ˜ å°„ã€‚

        Args:
            shapefile_path: è¼¸å…¥ GeoJSON æª”æ¡ˆçš„è·¯å¾‘
            output_csv: è¼¸å‡º CSV æª”æ¡ˆçš„è·¯å¾‘

        è™•ç†æ­¥é©Ÿï¼š
            1. è®€å– GeoJSON ä¸¦ä½¿ç”¨å‹•æ…‹ UTM å€é¸æ“‡è¨ˆç®—ä¸­å¿ƒé»
            2. æå–è¡Œæ”¿å€æ¬„ä½ï¼ˆsidonm, sggnm, adm_nmï¼‰
            3. è§£æ admin_3ï¼ˆå¾ adm_nm ç§»é™¤ sidonm å’Œ sggnmï¼‰
            4. ä½¿ç”¨ Wikidata ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ï¼ˆAdmin_1 å’Œ Admin_2ï¼‰
            5. ç”Ÿæˆæ¨™æº–åŒ– CSV

        Admin æ¬„ä½å¡«å……é‚è¼¯ï¼š
            - admin_1: å»£åŸŸå¸‚/é“ï¼ˆsidonm â†’ ç¹é«”ä¸­æ–‡ï¼Œå„ªå…ˆä½¿ç”¨å…§å»ºå°ç…§è¡¨ï¼‰
            - admin_2: å¸‚/å€/éƒ¡ï¼ˆsggnm â†’ ç¹é«”ä¸­æ–‡ï¼Œä½¿ç”¨ Wikidataï¼‰
            - admin_3: æ´/é‚‘/é¢ï¼ˆä¿ç•™éŸ“æ–‡åŸæ–‡ï¼‰
            - admin_4: ä¿æŒç©ºç™½

        Raises:
            Exception: GeoJSON è®€å–å¤±æ•—æˆ–è³‡æ–™è™•ç†éŒ¯èª¤æ™‚æ‹‹å‡º
        """
        try:
            logger.info(f"æ­£åœ¨è®€å– GeoJSON: {shapefile_path}")

            # === æ­¥é©Ÿ 1: è®€å– GeoJSON ä¸¦è¨ˆç®—ä¸­å¿ƒé» ===
            gdf = gpd.read_file(shapefile_path)
            logger.info(
                f"æˆåŠŸè®€å– GeoJSONï¼Œè³‡æ–™é›†å¤§å°: {gdf.shape[0]} è¡Œ x {gdf.shape[1]} åˆ—"
            )
            logger.info(f"åŸå§‹åº§æ¨™ç³»çµ±: {gdf.crs}")

            # ä½¿ç”¨å‹•æ…‹ UTM å€é¸æ“‡æ–¹æ³•ï¼ˆçµåˆ Albers æŠ•å½±ï¼‰è¨ˆç®—ä¸­å¿ƒé»
            # Reason: å—éŸ“æ©«è·¨å¤šå€‹ UTM å€ï¼ˆ51N, 52Nï¼‰ï¼Œ
            #         éœ€è¦æ ¹æ“šæ¯å€‹å¹¾ä½•é«”çš„å¯¦éš›ä½ç½®å‹•æ…‹é¸æ“‡ UTM å€ä»¥ç¢ºä¿ç²¾ç¢ºåº¦
            logger.info("ä½¿ç”¨æ–¹æ³•ï¼šå‹•æ…‹ UTM å€é¸æ“‡ï¼ˆçµåˆ Albers æŠ•å½±é€²è¡Œ UTM å€åˆ¤å®šï¼‰")
            gdf = self._calculate_centroids_utm(gdf)

            # ç§»é™¤ä¸éœ€è¦çš„å¹¾ä½•æ¬„ä½
            gdf = gdf.drop(columns=["geometry"])

            # çµ±ä¸€è³‡æ–™å‹æ…‹ï¼šå°‡ object é¡å‹è½‰ç‚ºå­—ä¸²ä¸¦å¡«å…… NaN
            for col in gdf.columns:
                if gdf[col].dtype == "object":
                    gdf[col] = gdf[col].fillna("").astype(str)

            # è½‰æ›ç‚º Polars DataFrame ä»¥é€²è¡Œé«˜æ•ˆçš„è³‡æ–™è™•ç†
            df = pl.from_pandas(gdf)

            # === æ­¥é©Ÿ 2: æå–ä¸¦è§£æè¡Œæ”¿å€æ¬„ä½ ===
            # å…ˆå»ºç«‹æ‰€éœ€çš„åŸºæœ¬æ¬„ä½
            df = df.select(
                [
                    pl.col("latitude"),
                    pl.col("longitude"),
                    pl.col("sidonm"),
                    pl.col("sggnm"),
                    pl.col("adm_nm"),
                ]
            )

            # è§£æ admin_3ï¼šå¾ adm_nm ç§»é™¤ sidonm å’Œ sggnm
            # Reason: Polars ä¸æ”¯æ´å‹•æ…‹æ¨¡å¼çš„ str.replaceï¼Œéœ€ä½¿ç”¨ apply æˆ–åˆ†æ­¥è™•ç†
            def extract_admin3(row):
                """å¾å®Œæ•´åœ°åä¸­æå– admin_3ï¼ˆæ´/é‚‘/é¢ï¼‰ã€‚"""
                adm_nm = row["adm_nm"]
                sidonm = row["sidonm"]
                sggnm = row["sggnm"]

                # ç§»é™¤ sidonm å’Œ sggnm
                result = adm_nm.replace(sidonm, "").replace(sggnm, "").strip()
                return result

            # ä½¿ç”¨ map_rows é€²è¡Œé€åˆ—è™•ç†
            df = df.with_columns(
                [
                    pl.struct(["adm_nm", "sidonm", "sggnm"])
                    .map_elements(
                        lambda row: row["adm_nm"]
                        .replace(row["sidonm"], "")
                        .replace(row["sggnm"], "")
                        .strip(),
                        return_dtype=pl.String,
                    )
                    .alias("admin_3")
                ]
            )

            # === æ­¥é©Ÿ 2.5: æ­£è¦åŒ–ç‰¹æ®Šè¡Œæ”¿å€çµæ§‹ï¼ˆä¸–å®—ç‰¹åˆ¥è‡ªæ²»å¸‚ï¼‰===
            df = self._normalize_special_admin_structures(df)

            # === æ­¥é©Ÿ 2.6: æ‹†åˆ†å¸‚ï¼‹å€çµ„åˆåç¨±ä¸¦èª¿æ•´å±¤ç´š ===
            df = self._normalize_city_district_hierarchy(df)

            # === æ­¥é©Ÿ 3: ä½¿ç”¨ Wikidata ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ ===
            logger.info("æ­£åœ¨åˆå§‹åŒ– Wikidata ç¿»è­¯å·¥å…·...")
            translator = WikidataTranslator(
                source_lang="ko",
                target_lang="zh-tw",
                fallback_langs=["zh-hant", "zh", "en", "ko"],
                cache_path="geoname_data/KR_wikidata_cache.json",
                use_opencc=True,
            )

            # å»ºç«‹å€™é¸éæ¿¾å™¨ï¼ˆç”¨æ–¼æ’é™¤è­°æœƒæ©Ÿæ§‹ç­‰éè¡Œæ”¿å€å¯¦é«”ï¼‰
            candidate_filter = self._build_candidate_filter()

            # æ­¥é©Ÿ 3.1: æ‰¹æ¬¡ç¿»è­¯ Admin_1ï¼ˆå»£åŸŸå¸‚/é“ï¼‰
            logger.info("æ­£åœ¨æ‰¹æ¬¡ç¿»è­¯ Admin_1ï¼ˆå»£åŸŸå¸‚/é“ï¼‰...")
            unique_admin1 = df["sidonm"].unique().to_list()
            admin1_qids = {}  # å„²å­˜ Admin_1 çš„ QID ç”¨æ–¼ P131 é©—è­‰

            # æ‰¹æ¬¡ç¿»è­¯æ‰€æœ‰ Admin_1ï¼ˆä¸»è¦ç‚ºæ‰¹æ¬¡å–å¾— QIDï¼‰
            admin1_translations = translator.batch_translate(
                unique_admin1, show_progress=True
            )

            # å¥—ç”¨å…§å»ºå°ç…§è¡¨è¦†è“‹ç¿»è­¯çµæœ
            for ko_name, result in admin1_translations.items():
                # Reason: ä½¿ç”¨å…§å»ºå°ç…§è¡¨å„ªå…ˆï¼Œç¢ºä¿ä½¿ç”¨å°ç£æ…£ç”¨ç°¡ç¨±
                if ko_name in self.ADMIN1_NAME_MAP:
                    admin1_qids[ko_name] = {
                        "translated": self.ADMIN1_NAME_MAP[ko_name],
                        "qid": result.get("qid"),
                    }
                else:
                    admin1_qids[ko_name] = {
                        "translated": result.get("translated", ko_name),
                        "qid": result.get("qid"),
                    }

            # æ­¥é©Ÿ 3.2: æŒ‰ Admin_1 åˆ†çµ„æ‰¹æ¬¡ç¿»è­¯ Admin_2ï¼ˆå¸‚/å€/éƒ¡ï¼‰
            logger.info("æ­£åœ¨æŒ‰ Admin_1 åˆ†çµ„æ‰¹æ¬¡ç¿»è­¯ Admin_2ï¼ˆå¸‚/å€/éƒ¡ï¼‰...")
            admin2_translations = {}

            # Reason: æŒ‰ Admin_1 åˆ†çµ„ç¿»è­¯ï¼Œç¢ºä¿åŒå Admin_2 ä½¿ç”¨æ­£ç¢ºçš„ parent QID
            for admin1_ko_name, admin1_data in admin1_qids.items():
                admin1_qid = admin1_data.get("qid")
                if not admin1_qid:
                    continue

                # å–å¾—æ­¤ Admin_1 ä¸‹çš„æ‰€æœ‰ Admin_2
                admin2_list = (
                    df.filter(pl.col("sidonm") == admin1_ko_name)["sggnm"]
                    .unique()
                    .to_list()
                )

                # ğŸ†• ç‰¹æ®Šè™•ç†ï¼šä¸–å®—ç‰¹åˆ¥è‡ªæ²»å¸‚ç›´æ¥ä½¿ç”¨æ‰‹å‹•å°ç…§è¡¨ï¼ˆå®Œå…¨è·³é Wikidataï¼‰
                if admin1_ko_name == "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ":
                    logger.info(
                        f"æ­£åœ¨è™•ç†ä¸–å®—ç‰¹åˆ¥è‡ªæ²»å¸‚çš„ {len(admin2_list)} å€‹ Admin_2"
                        f"ï¼ˆç›´æ¥ä½¿ç”¨æ‰‹å‹•å°ç…§è¡¨ï¼Œè·³é Wikidata æŸ¥è©¢ï¼‰..."
                    )

                    for korean_name in admin2_list:
                        if korean_name in self.SEJONG_ADMIN2_MAP:
                            # ç›´æ¥ä½¿ç”¨å°ç…§è¡¨ç¿»è­¯
                            admin2_translations[korean_name] = {
                                "translated": self.SEJONG_ADMIN2_MAP[korean_name],
                                "qid": "",  # ç„¡ QID
                                "source": "sejong_manual_map",  # æ¨™è¨˜ç‚ºä¸–å®—æ‰‹å‹•å°ç…§
                            }
                            logger.debug(
                                f"  {korean_name} â†’ {self.SEJONG_ADMIN2_MAP[korean_name]} (æ‰‹å‹•å°ç…§)"
                            )
                        else:
                            # ç†è«–ä¸Šä¸æ‡‰è©²ç™¼ç”Ÿï¼ˆå°ç…§è¡¨æ‡‰æ¶µè“‹æ‰€æœ‰ä¸–å®—åœ°åï¼‰
                            logger.warning(
                                f"  {korean_name} ä¸åœ¨æ‰‹å‹•å°ç…§è¡¨ä¸­ï¼Œä¿æŒåŸæ¨£"
                            )
                            admin2_translations[korean_name] = {
                                "translated": korean_name,
                                "qid": "",
                                "source": "missing_in_map",
                            }
                    continue  # è·³é Wikidata ç¿»è­¯æµç¨‹

                # å…¶ä»–åœ°å€ï¼šæ­£å¸¸ Wikidata ç¿»è­¯æµç¨‹
                logger.info(
                    f"æ­£åœ¨ç¿»è­¯ {admin1_data['translated']} ä¸‹çš„ {len(admin2_list)} å€‹ Admin_2..."
                )

                # ç‚ºé€™çµ„ Admin_2 å»ºç«‹çµ±ä¸€çš„ parent_qids
                parent_qids = {name: admin1_qid for name in admin2_list}

                # æ‰¹æ¬¡ç¿»è­¯é€™çµ„ Admin_2
                group_translations = translator.batch_translate(
                    admin2_list,
                    parent_qids=parent_qids,
                    show_progress=False,  # é¿å…é€²åº¦æ¢æ··äº‚
                    candidate_filter=candidate_filter,  # éæ¿¾è­°æœƒæ©Ÿæ§‹ç­‰éè¡Œæ”¿å€å¯¦é«”
                )

                # åˆä½µåˆ°ç¸½ç¿»è­¯çµæœ
                admin2_translations.update(group_translations)

            logger.info(f"Admin_2 ç¿»è­¯å®Œæˆï¼Œå…± {len(admin2_translations)} å€‹å”¯ä¸€åç¨±")

            # æ­¥é©Ÿ 3.3: å»ºç«‹å°ç…§å­—å…¸ä¸¦æ‡‰ç”¨åˆ° DataFrame
            logger.info("æ­£åœ¨æ‡‰ç”¨ç¿»è­¯çµæœ...")

            # å»ºç«‹ Admin_1 å°ç…§å­—å…¸
            admin1_map = {
                ko_name: data["translated"] for ko_name, data in admin1_qids.items()
            }

            # å»ºç«‹ Admin_2 å°ç…§å­—å…¸
            admin2_map = {
                ko_name: data.get("translated", ko_name)
                for ko_name, data in admin2_translations.items()
            }

            # æ‡‰ç”¨ç¿»è­¯åˆ° DataFrame
            df = df.with_columns(
                [
                    pl.col("sidonm")
                    .map_elements(
                        lambda x: admin1_map.get(x, x), return_dtype=pl.String
                    )
                    .alias("chinese_admin_1"),
                    pl.col("sggnm")
                    .map_elements(
                        lambda x: admin2_map.get(x, x), return_dtype=pl.String
                    )
                    .alias("chinese_admin_2"),
                    # Reason: Admin_3 ä¿ç•™éŸ“æ–‡åŸæ–‡ä»¥é™ä½ API è«‹æ±‚æ¬¡æ•¸
                    pl.col("admin_3").alias("chinese_admin_3"),
                ]
            )

            # é¡¯ç¤ºç¿»è­¯çµ±è¨ˆ
            logger.info(f"Admin_1 ç¿»è­¯æ•¸é‡: {len(admin1_qids)}")
            logger.info(f"Admin_2 ç¿»è­¯æ•¸é‡: {len(admin2_translations)}")
            logger.info("Admin_3 ä¿ç•™éŸ“æ–‡åŸæ–‡ï¼ˆæœªç¿»è­¯ï¼‰")

            # é‡çµ„ç‚ºæ¨™æº–æ ¼å¼
            df = df.select(
                [
                    pl.col("latitude"),
                    pl.col("longitude"),
                    pl.lit("å—éŸ“").alias("country"),  # åœ‹å®¶åç¨±
                    pl.col("chinese_admin_1").alias("admin_1"),  # ç¹é«”ä¸­æ–‡å»£åŸŸå¸‚/é“
                    pl.col("chinese_admin_2").alias("admin_2"),  # ç¹é«”ä¸­æ–‡å¸‚/å€/éƒ¡
                    pl.col("chinese_admin_3").alias("admin_3"),  # ç¹é«”ä¸­æ–‡æ´/é‚‘/é¢
                    pl.col("admin_4"),
                ]
            )

            # æ’åºï¼šä¾¿æ–¼ç‰ˆæœ¬æ§åˆ¶å·®ç•°æ¯”å°
            df = df.sort(["country", "admin_1", "admin_2"])

            # éæ¿¾ï¼šç§»é™¤ç„¡æ•ˆåº§æ¨™
            df = df.filter(
                pl.col("longitude").is_not_null() & pl.col("latitude").is_not_null()
            )

            # æ¨™æº–åŒ–åº§æ¨™ç²¾åº¦ï¼ˆé è¨­ 8 ä½å°æ•¸ï¼‰
            df = self.standardize_coordinate_precision(df)

            # å»ºç«‹è¼¸å‡ºç›®éŒ„ä¸¦å¯«å…¥ CSV
            output_path = Path(output_csv)
            output_path.parent.mkdir(parents=True, exist_ok=True)

            logger.info(f"æ­£åœ¨å„²å­˜ CSV æª”æ¡ˆ: {output_path}")
            df.write_csv(output_path)
            logger.info(f"æˆåŠŸå„²å­˜ CSV æª”æ¡ˆï¼Œå…± {len(df)} ç­†è³‡æ–™")

            # é¡¯ç¤ºå‰äº”ç­†ä¾›æª¢æŸ¥
            logger.info(df.head(5))

        except Exception as e:
            logger.error(f"è™•ç† GeoJSON æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise
